{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepRouteSet\n",
    "DeepRouteSet uses LSTM to generate new moonboard problem. It is modified from Coursera \"Improvise a Jazz Solo with an LSTM Network\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run the following cell to load all the packages required in this assignment. This may take a few minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import IPython\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Input, LSTM, Reshape, Lambda, RepeatVector\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import keras\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "import seaborn as sn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Problem statement\n",
    "\n",
    "You would like to create a jazz music piece specially for a friend's birthday. However, you don't know any instruments or music composition. Fortunately, you know deep learning and will solve this problem using an LSTM network.  \n",
    "\n",
    "You will train a network to generate novel jazz solos in a style representative of a body of performed work.\n",
    "\n",
    "<img src=\"images/jazz.jpg\" style=\"width:450;height:300px;\">\n",
    "\n",
    "\n",
    "### 1.1 - Dataset\n",
    "\n",
    "You will train your algorithm on a corpus of Jazz music. Run the cell below to listen to a snippet of the audio from the training set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have taken care of the preprocessing of the musical data to render it in terms of musical \"values.\" \n",
    "\n",
    "\n",
    "#### Music as a sequence of values\n",
    "* For the purpose of this assignment, all you need to know is that we will obtain a dataset of values, and will learn an RNN model to generate sequences of values. \n",
    "* Our music generation system will use 237 unique values. \n",
    "\n",
    "Run the following code to load the raw music data and preprocess it into values. This might take a few minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Path to read handString_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "parent_wd = cwd.replace('/model', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_handString_seq_path = parent_wd + '/preprocessing/benchmark_handString_seq_X'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(benchmark_handString_seq_path, 'rb') as f:\n",
    "    benchmark_handString_seq = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orginal handString looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B4-LH', 'D6-RH', 'F9-LH', 'G11-RH', 'C15-LH', 'H18-RH']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_handString_seq[\"189344\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training example:  (358,)\n"
     ]
    }
   ],
   "source": [
    "## ensemble to a StringList\n",
    "handStringList = []\n",
    "for key in benchmark_handString_seq.keys():\n",
    "    handStringList.append(benchmark_handString_seq[key])\n",
    "print(\"Number of training example: \", np.shape(handStringList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total holds avalible (include L / R):  236\n"
     ]
    }
   ],
   "source": [
    "# Merge all string to big list to know how many string to consider\n",
    "holdsReservoir = [] \n",
    "for i in range(len(handStringList)):\n",
    "    holdsReservoir = holdsReservoir + handStringList[i]\n",
    "#holdsReservoir = sorted(holdsReservoir)  It will be great to sort the num of String from bottom to top.  \n",
    "holdsReservoir = list(set(holdsReservoir)) # Delete repetitive string\n",
    "print('Total holds avalible (include L / R): ', len(holdsReservoir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For example, \"J5-LH\" has index =  182\n"
     ]
    }
   ],
   "source": [
    "# Build a dictionary convert String \"J5-LH\" to index\n",
    "holdStr_to_holdIx = {}\n",
    "holdStr_to_holdIx[\"End\"] = 0  # End hold\n",
    "for i in range(len(holdsReservoir)):\n",
    "    holdStr_to_holdIx[holdsReservoir[i]] = i + 1\n",
    "print('For example, \"J5-LH\" has index = ', holdStr_to_holdIx[\"J5-LH\"])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdIx_to_holdStr = {v: k for k, v in holdStr_to_holdIx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reverse dictionary: J5-LH\n"
     ]
    }
   ],
   "source": [
    "print('Reverse dictionary:', holdIx_to_holdStr[182])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loadSeqXYFromString "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadSeqXYFromString (stringList, holdStr_to_holdIx, m = 358, maxNumOfHands = 12, numOfPossibleHolds = 236):\n",
    "    \"\"\"Input with HandSting list ['J5-LH', 'J5-RH', 'I9-LH', 'J10-RH', 'H12-RH', 'C13-LH', 'D15-LH', 'E18-RH']\n",
    "       Different training sample have different length, so padded with 0 (\"End\") up to maxNumOfHands\n",
    "       OutPut X, Y HandString matrix to feed in RNN\n",
    "       OutPut shape X (Training sample, Tx, numOfPossibleHolds + \"End\") = (358, 12, 237)\n",
    "       OutPut shape Y (Tx, Training sample, numOfPossibleHolds + \"End\") = (12, 358, 237)\n",
    "    \"\"\"\n",
    "    n_values = numOfPossibleHolds + 1 # including \"End\"\n",
    "    X = np.zeros((m, maxNumOfHands, n_values), dtype=np.bool)\n",
    "    Y = np.zeros((m, maxNumOfHands, n_values), dtype=np.bool)\n",
    "    for ixOfSample in range(m):\n",
    "        # Extract a seq like ['J5-LH', 'J5-RH', 'I9-LH', 'J10-RH', 'H12-RH', 'C13-LH', 'D15-LH', 'E18-RH']\n",
    "        one_Seq = stringList[ixOfSample]\n",
    "        \n",
    "        # Convert each string to index\n",
    "        ixList = [holdStr_to_holdIx[string] for string in one_Seq]\n",
    "        \n",
    "        # Specify one hot X, Y and take care of padding. Note that Y[n] = X[n+1]\n",
    "        # Pad 0 after the end hold up to maxNumOfHands\n",
    "        for j in range(maxNumOfHands):\n",
    "            if j >= len(ixList): # condition to pad\n",
    "                Y[ixOfSample, j, 0] = 1 # Pad 0\n",
    "                if j+1 < maxNumOfHands:\n",
    "                    X[ixOfSample, j+1, 0] = 1  # Pad 0          \n",
    "            else:\n",
    "                idx = ixList[j]  # Y[n] = X[n+1]\n",
    "                X[ixOfSample, j+1, idx] = 1\n",
    "                Y[ixOfSample, j, idx] = 1\n",
    "                \n",
    "    Y = np.swapaxes(Y,0,1)  # Y is different shape than X\n",
    "    Y = Y.tolist()\n",
    "    return np.asarray(X), np.asarray(Y), n_values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, n_values = loadSeqXYFromString (handStringList, holdStr_to_holdIx, maxNumOfHands = 12, numOfPossibleHolds = 236)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False  True False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False]\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False  True False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "# Check the element of Y[n] = X[n+1]. X[sample, hand=0, :]\n",
    "print(X[50,5,:])\n",
    "print(Y[4,50,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples: 358\n",
      "Tx (length of sequence): 12\n",
      "total # of unique values: 237\n",
      "shape of X: (358, 12, 237)\n",
      "Shape of Y: (12, 358, 237)\n"
     ]
    }
   ],
   "source": [
    "X, Y, n_values = loadSeqXYFromString (handStringList, holdStr_to_holdIx, maxNumOfHands = 12, numOfPossibleHolds = 236)\n",
    "print('number of training examples:', X.shape[0])\n",
    "print('Tx (length of sequence):', X.shape[1])\n",
    "print('total # of unique values:', n_values)\n",
    "print('shape of X:', X.shape)\n",
    "print('Shape of Y:', Y.shape)\n",
    "#print('First training sample, 4 holds\\'s one hot', X[0,4,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have just loaded the following:\n",
    "\n",
    "- `X`: This is an (m, $T_x$, 237) dimensional array. \n",
    "    - We have m training examples, each of which is a snippet of $T_x =30$ musical values. \n",
    "    - At each time step, the input is one of 237 different possible values, represented as a one-hot vector. \n",
    "        - For example, X[i,t,:] is a one-hot vector representing the value of the i-th example at time t. \n",
    "\n",
    "- `Y`: a $(T_y, m, 237)$ dimensional array\n",
    "    - This is essentially the same as `X`, but shifted one step to the left (to the past). \n",
    "    - Notice that the data in `Y` is **reordered** to be dimension $(T_y, m, 237)$, where $T_y = T_x$. This format makes it more convenient to feed into the LSTM later.\n",
    "    - Similar to the dinosaur assignment, we're using the previous values to predict the next value.\n",
    "        - So our sequence model will try to predict $y^{\\langle t \\rangle}$ given $x^{\\langle 1\\rangle}, \\ldots, x^{\\langle t \\rangle}$. \n",
    "\n",
    "- `n_values`: The number of unique values in this dataset. This should be 237. \n",
    "\n",
    "- `indices_values`: python dictionary mapping integers 0 through 77 to musical values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Overview of our model\n",
    "\n",
    "Here is the architecture of the model we will use. This is similar to the Dinosaurus model, except that you will implement it in Keras.\n",
    "\n",
    "<img src=\"images/music_generation.png\" style=\"width:600;height:400px;\">\n",
    "\n",
    "\n",
    "* $X = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, \\cdots, x^{\\langle T_x \\rangle})$ is a window of size $T_x$ scanned over the musical corpus. \n",
    "* Each $x^{\\langle t \\rangle}$ is an index corresponding to a value.\n",
    "* $\\hat{y}^{t}$ is the prediction for the next value.\n",
    "* We will be training the model on random snippets of 30 values taken from a much longer piece of music. \n",
    "    - Thus, we won't bother to set the first input $x^{\\langle 1 \\rangle} = \\vec{0}$, since most of these snippets of audio start somewhere in the middle of a piece of music. \n",
    "    - We are setting each of the snippets to have the same length $T_x = 30$ to make vectorization easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of parts 2 and 3\n",
    "\n",
    "* We're going to train a model that predicts the next note in a style that is similar to the jazz music that it's trained on.  The training is contained in the weights and biases of the model. \n",
    "* In Part 3, we're then going to use those weights and biases in a new model which predicts a series of notes, using the previous note to predict the next note. \n",
    "* The weights and biases are transferred to the new model using 'global shared layers' described below\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Building the model\n",
    "\n",
    "* In this part you will build and train a model that will learn musical patterns. \n",
    "* The model takes input X of shape $(m, T_x, 237)$ and labels Y of shape $(T_y, m, 237)$. \n",
    "* We will use an LSTM with hidden states that have $n_{a} = 64$ dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of dimensions for the hidden state of each LSTM cell.\n",
    "n_a = 64 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Sequence generation uses a for-loop\n",
    "* If you're building an RNN where, at test time, the entire input sequence $x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, \\ldots, x^{\\langle T_x \\rangle}$ is given in advance, then Keras has simple built-in functions to build the model. \n",
    "* However, for **sequence generation, at test time we don't know all the values of $x^{\\langle t\\rangle}$ in advance**.\n",
    "* Instead we generate them one at a time using $x^{\\langle t\\rangle} = y^{\\langle t-1 \\rangle}$. \n",
    "    * The input at time \"t\" is the prediction at the previous time step \"t-1\".\n",
    "* So you'll need to implement your own for-loop to iterate over the time steps. \n",
    "\n",
    "#### Shareable weights\n",
    "* The function `djmodel()` will call the LSTM layer $T_x$ times using a for-loop.\n",
    "* It is important that all $T_x$ copies have the same weights. \n",
    "    - The $T_x$ steps should have shared weights that aren't re-initialized.\n",
    "* Referencing a globally defined shared layer will utilize the same layer-object instance at each time step.\n",
    "* The key steps for implementing layers with shareable weights in Keras are: \n",
    "1. Define the layer objects (we will use global variables for this).\n",
    "2. Call these objects when propagating the input.\n",
    "\n",
    "#### 3 types of layers\n",
    "* We have defined the layers objects you need as global variables.  \n",
    "* Please run the next cell to create them. \n",
    "* Please read the Keras documentation and understand these layers: \n",
    "    - [Reshape()](https://keras.io/layers/core/#reshape): Reshapes an output to a certain shape.\n",
    "    - [LSTM()](https://keras.io/layers/recurrent/#lstm): Long Short-Term Memory layer\n",
    "    - [Dense()](https://keras.io/layers/core/#dense): A regular fully-connected neural network layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_values = 237 # number of music values\n",
    "reshapor = Reshape((1, n_values))                        # Used in Step 2.B of djmodel(), below\n",
    "LSTM_cell = LSTM(n_a, return_state = True)         # Used in Step 2.C\n",
    "densor = Dense(n_values, activation='softmax')     # Used in Step 2.D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `reshapor`, `LSTM_cell` and `densor` are globally defined layer objects, that you'll use to implement `djmodel()`. \n",
    "* In order to propagate a Keras tensor object X through one of these layers, use `layer_object()`.\n",
    "    - For one input, use `layer_object(X)`\n",
    "    - For more than one input, put the inputs in a list: `layer_object([X1,X2])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Exercise**: Implement `djmodel()`. \n",
    "\n",
    "#### Inputs (given)\n",
    "* The `Input()` layer is used for defining the input `X` as well as the initial hidden state 'a0' and cell state `c0`.\n",
    "* The `shape` parameter takes a tuple that does not include the batch dimension (`m`).\n",
    "    - For example,\n",
    "    ```Python\n",
    "    X = Input(shape=(Tx, n_values)) # X has 3 dimensions and not 2: (m, Tx, n_values)\n",
    "    ```\n",
    "#### Step 1: Outputs (TODO)\n",
    "1. Create an empty list \"outputs\" to save the outputs of the LSTM Cell at every time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Loop through time steps (TODO)\n",
    "* Loop for $t \\in 1, \\ldots, T_x$:\n",
    "\n",
    "#### 2A. Select the 't' time-step vector from X.\n",
    "* X has the shape (m, Tx, n_values).\n",
    "* The shape of the 't' selection should be (n_values,). \n",
    "* Recall that if you were implementing in numpy instead of Keras, you would extract a slice from a 3D numpy array like this:\n",
    "```Python\n",
    "var1 = array1[:,1,:]\n",
    "```\n",
    "    \n",
    "#### Lambda layer\n",
    "* Since we're using Keras, we need to define this step inside a custom layer.\n",
    "* In Keras, this is a Lambda layer [Lambda](https://keras.io/layers/core/#lambda)\n",
    "* As an example, a Lambda layer that takes the previous layer and adds '1' looks like this\n",
    "```    \n",
    "       lambda_layer1 = Lambda(lambda z: z + 1)(previous_layer)\n",
    "``` \n",
    "* The previous layer in this case is `X`.\n",
    "* `z` is a local variable of the lambda function. \n",
    "    * The `previous_layer` gets passed into the parameter `z` in the lowercase `lambda` function.\n",
    "    * You can choose the name of the variable to be something else if you want.\n",
    "* The operation after the colon ':' should be the operation to extract a slice from the previous layer.\n",
    "* **Hint**: You'll be using the variable `t` within the definition of the lambda layer even though it isn't passed in as an argument to Lambda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2B. Reshape x to be (1,n_values).\n",
    "* Use the `reshapor()` layer.  It is a function that takes the previous layer as its input argument.\n",
    "\n",
    "#### 2C. Run x through one step of LSTM_cell.\n",
    "* Initialize the `LSTM_cell` with the previous step's hidden state $a$ and cell state $c$. \n",
    "* Use the following formatting:\n",
    "```python\n",
    "next_hidden_state, _, next_cell_state = LSTM_cell(inputs=input_x, initial_state=[previous_hidden_state, previous_cell_state])\n",
    "```\n",
    "    * Choose appropriate variables for inputs, hidden state and cell state.\n",
    "\n",
    "#### 2D. Dense layer\n",
    "* Propagate the LSTM's hidden state through a dense+softmax layer using `densor`. \n",
    "    \n",
    "#### 2E. Append output\n",
    "* Append the output to the list of \"outputs\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: After the loop, create the model\n",
    "* Use the Keras `Model` object to create a model.\n",
    "* specify the inputs and outputs:\n",
    "```Python\n",
    "model = Model(inputs=[input_x, initial_hidden_state, initial_cell_state], outputs=the_outputs)\n",
    "```\n",
    "    * Choose the appropriate variables for the input tensor, hidden state, cell state, and output.\n",
    "* See the documentation for [Model](https://keras.io/models/model/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: djmodel\n",
    "\n",
    "def djmodel(Tx, n_a, n_values):\n",
    "    \"\"\"\n",
    "    Implement the model\n",
    "    \n",
    "    Arguments:\n",
    "    Tx -- length of the sequence in a corpus\n",
    "    n_a -- the number of activations used in our model\n",
    "    n_values -- number of unique values in the music data \n",
    "    \n",
    "    Returns:\n",
    "    model -- a keras instance model with n_a activations\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input layer and specify the shape\n",
    "    X = Input(shape=(Tx, n_values))\n",
    "    \n",
    "    # Define the initial hidden state a0 and initial cell state c0\n",
    "    # using `Input`\n",
    "    a0 = Input(shape=(n_a,), name='a0')\n",
    "    c0 = Input(shape=(n_a,), name='c0')\n",
    "    a = a0\n",
    "    c = c0\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    # Step 1: Create empty list to append the outputs while you iterate (≈1 line)\n",
    "    outputs = []\n",
    "    \n",
    "    # Step 2: Loop\n",
    "    for t in range(Tx):\n",
    "        \n",
    "        # Step 2.A: select the \"t\"th time step vector from X. \n",
    "        x = Lambda(lambda z: z[:, t, :])(X)   \n",
    "\n",
    "        # Step 2.B: Use reshapor to reshape x to be (1, n_values) (≈1 line)\n",
    "        x = reshapor(x)  # from (?, 237) to (?, 1, 237)\n",
    "    \n",
    "        # Step 2.C: Perform one step of the LSTM_cell\n",
    "        a, _, c = LSTM_cell(inputs = x, initial_state = [a, c])\n",
    "        # Step 2.D: Apply densor to the hidden state output of LSTM_Cell\n",
    "        out = densor(a)\n",
    "        # Step 2.E: add the output to \"outputs\"\n",
    "        outputs.append(out)\n",
    "        \n",
    "    # Step 3: Create model instance\n",
    "    model = Model(inputs = [X, a0, c0], outputs = outputs)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model object\n",
    "* Run the following cell to define your model. \n",
    "* We will use `Tx=30`, `n_a=64` (the dimension of the LSTM activations), and `n_values=237`. \n",
    "* This cell may take a few seconds to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = djmodel(Tx = 12 , n_a = 64, n_values = 237)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 12, 237)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 237)          0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 237)       0           lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "                                                                 lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "                                                                 lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "                                                                 lambda_8[0][0]                   \n",
      "                                                                 lambda_9[0][0]                   \n",
      "                                                                 lambda_10[0][0]                  \n",
      "                                                                 lambda_11[0][0]                  \n",
      "                                                                 lambda_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "a0 (InputLayer)                 (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 237)          0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 64), (None,  77312       reshape_1[0][0]                  \n",
      "                                                                 a0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 reshape_1[1][0]                  \n",
      "                                                                 lstm_1[0][0]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "                                                                 reshape_1[2][0]                  \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[1][2]                     \n",
      "                                                                 reshape_1[3][0]                  \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[2][2]                     \n",
      "                                                                 reshape_1[4][0]                  \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[3][2]                     \n",
      "                                                                 reshape_1[5][0]                  \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[4][2]                     \n",
      "                                                                 reshape_1[6][0]                  \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[5][2]                     \n",
      "                                                                 reshape_1[7][0]                  \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[6][2]                     \n",
      "                                                                 reshape_1[8][0]                  \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[7][2]                     \n",
      "                                                                 reshape_1[9][0]                  \n",
      "                                                                 lstm_1[8][0]                     \n",
      "                                                                 lstm_1[8][2]                     \n",
      "                                                                 reshape_1[10][0]                 \n",
      "                                                                 lstm_1[9][0]                     \n",
      "                                                                 lstm_1[9][2]                     \n",
      "                                                                 reshape_1[11][0]                 \n",
      "                                                                 lstm_1[10][0]                    \n",
      "                                                                 lstm_1[10][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 237)          0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 237)          0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 237)          0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 237)          0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 237)          0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 237)          0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 237)          0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 237)          0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 237)          0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 237)          0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 237)          15405       lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[8][0]                     \n",
      "                                                                 lstm_1[9][0]                     \n",
      "                                                                 lstm_1[10][0]                    \n",
      "                                                                 lstm_1[11][0]                    \n",
      "==================================================================================================\n",
      "Total params: 92,717\n",
      "Trainable params: 92,717\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Check your model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the model for training\n",
    "* You now need to compile your model to be trained. \n",
    "* We will use:\n",
    "    - optimizer: Adam optimizer\n",
    "    - Loss function: categorical cross-entropy (for multi-class classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize hidden state and cell state\n",
    "Finally, let's initialize `a0` and `c0` for the LSTM's initial state to be zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = 358\n",
    "a0 = np.zeros((m, n_a))\n",
    "c0 = np.zeros((m, n_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model\n",
    "* Lets now fit the model! \n",
    "* We will turn `Y` into a list, since the cost function expects `Y` to be provided in this format \n",
    "    - `list(Y)` is a list with 30 items, where each of the list items is of shape (358,237). \n",
    "    - Lets train for 100 epochs. This will take a few minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "358/358 [==============================] - 4s 10ms/step - loss: 50.3024 - dense_1_loss: 1.9220 - dense_1_accuracy: 0.0000e+00 - dense_1_accuracy_1: 0.0196 - dense_1_accuracy_2: 0.0000e+00 - dense_1_accuracy_3: 0.0000e+00 - dense_1_accuracy_4: 0.0000e+00 - dense_1_accuracy_5: 0.0391 - dense_1_accuracy_6: 0.2654 - dense_1_accuracy_7: 0.6229 - dense_1_accuracy_8: 0.8575 - dense_1_accuracy_9: 0.9050 - dense_1_accuracy_10: 0.9078 - dense_1_accuracy_11: 0.9106                   \n",
      "Epoch 2/100\n",
      "358/358 [==============================] - 0s 300us/step - loss: 39.5153 - dense_1_loss: 0.4104 - dense_1_accuracy: 0.0000e+00 - dense_1_accuracy_1: 0.0419 - dense_1_accuracy_2: 0.0000e+00 - dense_1_accuracy_3: 0.0000e+00 - dense_1_accuracy_4: 0.0000e+00 - dense_1_accuracy_5: 0.0391 - dense_1_accuracy_6: 0.3017 - dense_1_accuracy_7: 0.6844 - dense_1_accuracy_8: 0.9358 - dense_1_accuracy_9: 0.9916 - dense_1_accuracy_10: 0.9972 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 3/100\n",
      "358/358 [==============================] - 0s 286us/step - loss: 37.0568 - dense_1_loss: 0.0745 - dense_1_accuracy: 0.0000e+00 - dense_1_accuracy_1: 0.1955 - dense_1_accuracy_2: 0.0000e+00 - dense_1_accuracy_3: 0.0000e+00 - dense_1_accuracy_4: 0.0000e+00 - dense_1_accuracy_5: 0.0391 - dense_1_accuracy_6: 0.3017 - dense_1_accuracy_7: 0.6844 - dense_1_accuracy_8: 0.9358 - dense_1_accuracy_9: 0.9916 - dense_1_accuracy_10: 0.9972 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 4/100\n",
      "358/358 [==============================] - 0s 287us/step - loss: 35.9723 - dense_1_loss: 0.0264 - dense_1_accuracy: 0.0000e+00 - dense_1_accuracy_1: 0.2514 - dense_1_accuracy_2: 0.0000e+00 - dense_1_accuracy_3: 0.0000e+00 - dense_1_accuracy_4: 0.0000e+00 - dense_1_accuracy_5: 0.0391 - dense_1_accuracy_6: 0.3017 - dense_1_accuracy_7: 0.6844 - dense_1_accuracy_8: 0.9358 - dense_1_accuracy_9: 0.9916 - dense_1_accuracy_10: 0.9972 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 5/100\n",
      "358/358 [==============================] - 0s 297us/step - loss: 34.9940 - dense_1_loss: 0.0381 - dense_1_accuracy: 0.0000e+00 - dense_1_accuracy_1: 0.2039 - dense_1_accuracy_2: 0.0000e+00 - dense_1_accuracy_3: 0.0140 - dense_1_accuracy_4: 0.0000e+00 - dense_1_accuracy_5: 0.0419 - dense_1_accuracy_6: 0.3017 - dense_1_accuracy_7: 0.6844 - dense_1_accuracy_8: 0.9358 - dense_1_accuracy_9: 0.9916 - dense_1_accuracy_10: 0.9972 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 6/100\n",
      "358/358 [==============================] - 0s 307us/step - loss: 34.0047 - dense_1_loss: 0.0294 - dense_1_accuracy: 0.0000e+00 - dense_1_accuracy_1: 0.1788 - dense_1_accuracy_2: 0.0000e+00 - dense_1_accuracy_3: 0.0419 - dense_1_accuracy_4: 0.0140 - dense_1_accuracy_5: 0.0531 - dense_1_accuracy_6: 0.3045 - dense_1_accuracy_7: 0.6844 - dense_1_accuracy_8: 0.9358 - dense_1_accuracy_9: 0.9916 - dense_1_accuracy_10: 0.9972 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 7/100\n",
      "358/358 [==============================] - 0s 301us/step - loss: 32.9651 - dense_1_loss: 0.0215 - dense_1_accuracy: 0.0000e+00 - dense_1_accuracy_1: 0.1788 - dense_1_accuracy_2: 0.0000e+00 - dense_1_accuracy_3: 0.0419 - dense_1_accuracy_4: 0.0363 - dense_1_accuracy_5: 0.0726 - dense_1_accuracy_6: 0.3045 - dense_1_accuracy_7: 0.6844 - dense_1_accuracy_8: 0.9358 - dense_1_accuracy_9: 0.9916 - dense_1_accuracy_10: 0.9972 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 8/100\n",
      "358/358 [==============================] - 0s 310us/step - loss: 31.9764 - dense_1_loss: 0.0181 - dense_1_accuracy: 0.0000e+00 - dense_1_accuracy_1: 0.1899 - dense_1_accuracy_2: 0.0000e+00 - dense_1_accuracy_3: 0.0726 - dense_1_accuracy_4: 0.0447 - dense_1_accuracy_5: 0.0978 - dense_1_accuracy_6: 0.3017 - dense_1_accuracy_7: 0.6844 - dense_1_accuracy_8: 0.9358 - dense_1_accuracy_9: 0.9916 - dense_1_accuracy_10: 0.9972 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 9/100\n",
      "358/358 [==============================] - 0s 310us/step - loss: 31.0816 - dense_1_loss: 0.0166 - dense_1_accuracy: 0.0000e+00 - dense_1_accuracy_1: 0.3939 - dense_1_accuracy_2: 0.0000e+00 - dense_1_accuracy_3: 0.0838 - dense_1_accuracy_4: 0.0419 - dense_1_accuracy_5: 0.1006 - dense_1_accuracy_6: 0.3073 - dense_1_accuracy_7: 0.6844 - dense_1_accuracy_8: 0.9358 - dense_1_accuracy_9: 0.9916 - dense_1_accuracy_10: 0.9972 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 10/100\n",
      "358/358 [==============================] - 0s 307us/step - loss: 29.9951 - dense_1_loss: 0.0128 - dense_1_accuracy: 0.1229 - dense_1_accuracy_1: 0.4497 - dense_1_accuracy_2: 0.0168 - dense_1_accuracy_3: 0.0866 - dense_1_accuracy_4: 0.0503 - dense_1_accuracy_5: 0.1313 - dense_1_accuracy_6: 0.3101 - dense_1_accuracy_7: 0.6844 - dense_1_accuracy_8: 0.9385 - dense_1_accuracy_9: 0.9916 - dense_1_accuracy_10: 0.9972 - dense_1_accuracy_11: 1.0000     \n",
      "Epoch 11/100\n",
      "358/358 [==============================] - 0s 300us/step - loss: 28.9830 - dense_1_loss: 0.0099 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.4693 - dense_1_accuracy_2: 0.0335 - dense_1_accuracy_3: 0.0922 - dense_1_accuracy_4: 0.0615 - dense_1_accuracy_5: 0.1620 - dense_1_accuracy_6: 0.3128 - dense_1_accuracy_7: 0.6872 - dense_1_accuracy_8: 0.9358 - dense_1_accuracy_9: 0.9916 - dense_1_accuracy_10: 0.9972 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 12/100\n",
      "358/358 [==============================] - 0s 292us/step - loss: 28.0921 - dense_1_loss: 0.0128 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.5140 - dense_1_accuracy_2: 0.0531 - dense_1_accuracy_3: 0.1369 - dense_1_accuracy_4: 0.1034 - dense_1_accuracy_5: 0.1788 - dense_1_accuracy_6: 0.3240 - dense_1_accuracy_7: 0.6872 - dense_1_accuracy_8: 0.9358 - dense_1_accuracy_9: 0.9916 - dense_1_accuracy_10: 0.9972 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 13/100\n",
      "358/358 [==============================] - 0s 303us/step - loss: 27.2142 - dense_1_loss: 0.0077 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.5140 - dense_1_accuracy_2: 0.0950 - dense_1_accuracy_3: 0.1788 - dense_1_accuracy_4: 0.1089 - dense_1_accuracy_5: 0.2151 - dense_1_accuracy_6: 0.3380 - dense_1_accuracy_7: 0.6927 - dense_1_accuracy_8: 0.9385 - dense_1_accuracy_9: 0.9916 - dense_1_accuracy_10: 0.9972 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 14/100\n",
      "358/358 [==============================] - 0s 307us/step - loss: 26.4003 - dense_1_loss: 0.0060 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.5419 - dense_1_accuracy_2: 0.0922 - dense_1_accuracy_3: 0.1676 - dense_1_accuracy_4: 0.1117 - dense_1_accuracy_5: 0.1844 - dense_1_accuracy_6: 0.3575 - dense_1_accuracy_7: 0.6927 - dense_1_accuracy_8: 0.9358 - dense_1_accuracy_9: 0.9916 - dense_1_accuracy_10: 0.9972 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 15/100\n",
      "358/358 [==============================] - 0s 297us/step - loss: 25.5604 - dense_1_loss: 0.0050 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.5475 - dense_1_accuracy_2: 0.1201 - dense_1_accuracy_3: 0.2011 - dense_1_accuracy_4: 0.1285 - dense_1_accuracy_5: 0.2095 - dense_1_accuracy_6: 0.3799 - dense_1_accuracy_7: 0.7095 - dense_1_accuracy_8: 0.9413 - dense_1_accuracy_9: 0.9916 - dense_1_accuracy_10: 0.9972 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 16/100\n",
      "358/358 [==============================] - 0s 301us/step - loss: 24.7960 - dense_1_loss: 0.0043 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.5698 - dense_1_accuracy_2: 0.1061 - dense_1_accuracy_3: 0.1983 - dense_1_accuracy_4: 0.1620 - dense_1_accuracy_5: 0.2458 - dense_1_accuracy_6: 0.4106 - dense_1_accuracy_7: 0.7318 - dense_1_accuracy_8: 0.9385 - dense_1_accuracy_9: 0.9916 - dense_1_accuracy_10: 0.9972 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 17/100\n",
      "358/358 [==============================] - 0s 304us/step - loss: 24.0636 - dense_1_loss: 0.0040 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.5950 - dense_1_accuracy_2: 0.1285 - dense_1_accuracy_3: 0.2123 - dense_1_accuracy_4: 0.1564 - dense_1_accuracy_5: 0.2709 - dense_1_accuracy_6: 0.4721 - dense_1_accuracy_7: 0.7542 - dense_1_accuracy_8: 0.9497 - dense_1_accuracy_9: 0.9916 - dense_1_accuracy_10: 0.9972 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 18/100\n",
      "358/358 [==============================] - 0s 302us/step - loss: 23.4859 - dense_1_loss: 0.0027 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.6034 - dense_1_accuracy_2: 0.1425 - dense_1_accuracy_3: 0.2207 - dense_1_accuracy_4: 0.1704 - dense_1_accuracy_5: 0.2682 - dense_1_accuracy_6: 0.4637 - dense_1_accuracy_7: 0.7402 - dense_1_accuracy_8: 0.9469 - dense_1_accuracy_9: 0.9916 - dense_1_accuracy_10: 0.9972 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 19/100\n",
      "358/358 [==============================] - 0s 300us/step - loss: 22.7958 - dense_1_loss: 0.0023 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.6061 - dense_1_accuracy_2: 0.1453 - dense_1_accuracy_3: 0.2263 - dense_1_accuracy_4: 0.1872 - dense_1_accuracy_5: 0.3128 - dense_1_accuracy_6: 0.4972 - dense_1_accuracy_7: 0.7933 - dense_1_accuracy_8: 0.9553 - dense_1_accuracy_9: 0.9916 - dense_1_accuracy_10: 0.9972 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 20/100\n",
      "358/358 [==============================] - 0s 293us/step - loss: 22.3277 - dense_1_loss: 0.0023 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.6285 - dense_1_accuracy_2: 0.1648 - dense_1_accuracy_3: 0.2402 - dense_1_accuracy_4: 0.1927 - dense_1_accuracy_5: 0.3240 - dense_1_accuracy_6: 0.5084 - dense_1_accuracy_7: 0.7793 - dense_1_accuracy_8: 0.9525 - dense_1_accuracy_9: 0.9916 - dense_1_accuracy_10: 0.9972 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 21/100\n",
      "358/358 [==============================] - 0s 294us/step - loss: 21.7485 - dense_1_loss: 0.0018 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.6285 - dense_1_accuracy_2: 0.1592 - dense_1_accuracy_3: 0.2263 - dense_1_accuracy_4: 0.1955 - dense_1_accuracy_5: 0.3128 - dense_1_accuracy_6: 0.5223 - dense_1_accuracy_7: 0.8073 - dense_1_accuracy_8: 0.9525 - dense_1_accuracy_9: 0.9916 - dense_1_accuracy_10: 0.9972 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 22/100\n",
      "358/358 [==============================] - 0s 301us/step - loss: 21.1080 - dense_1_loss: 0.0023 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.6536 - dense_1_accuracy_2: 0.1788 - dense_1_accuracy_3: 0.2514 - dense_1_accuracy_4: 0.2374 - dense_1_accuracy_5: 0.3436 - dense_1_accuracy_6: 0.5447 - dense_1_accuracy_7: 0.7989 - dense_1_accuracy_8: 0.9609 - dense_1_accuracy_9: 0.9916 - dense_1_accuracy_10: 0.9972 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 23/100\n",
      "358/358 [==============================] - 0s 303us/step - loss: 20.6329 - dense_1_loss: 0.0016 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.6732 - dense_1_accuracy_2: 0.1927 - dense_1_accuracy_3: 0.2514 - dense_1_accuracy_4: 0.2318 - dense_1_accuracy_5: 0.3464 - dense_1_accuracy_6: 0.5726 - dense_1_accuracy_7: 0.8156 - dense_1_accuracy_8: 0.9553 - dense_1_accuracy_9: 0.9916 - dense_1_accuracy_10: 0.9972 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 24/100\n",
      "358/358 [==============================] - 0s 281us/step - loss: 20.1518 - dense_1_loss: 0.0019 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.6844 - dense_1_accuracy_2: 0.1899 - dense_1_accuracy_3: 0.2654 - dense_1_accuracy_4: 0.2486 - dense_1_accuracy_5: 0.3575 - dense_1_accuracy_6: 0.5810 - dense_1_accuracy_7: 0.8296 - dense_1_accuracy_8: 0.9665 - dense_1_accuracy_9: 0.9916 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 25/100\n",
      "358/358 [==============================] - 0s 286us/step - loss: 19.6848 - dense_1_loss: 0.0017 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.6899 - dense_1_accuracy_2: 0.1872 - dense_1_accuracy_3: 0.2682 - dense_1_accuracy_4: 0.2793 - dense_1_accuracy_5: 0.3911 - dense_1_accuracy_6: 0.5950 - dense_1_accuracy_7: 0.8296 - dense_1_accuracy_8: 0.9665 - dense_1_accuracy_9: 0.9916 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 26/100\n",
      "358/358 [==============================] - 0s 299us/step - loss: 19.2278 - dense_1_loss: 0.0013 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7095 - dense_1_accuracy_2: 0.1927 - dense_1_accuracy_3: 0.2709 - dense_1_accuracy_4: 0.2765 - dense_1_accuracy_5: 0.4050 - dense_1_accuracy_6: 0.6201 - dense_1_accuracy_7: 0.8380 - dense_1_accuracy_8: 0.9637 - dense_1_accuracy_9: 0.9916 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 27/100\n",
      "358/358 [==============================] - 0s 298us/step - loss: 18.8215 - dense_1_loss: 0.0014 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7123 - dense_1_accuracy_2: 0.2039 - dense_1_accuracy_3: 0.2877 - dense_1_accuracy_4: 0.3073 - dense_1_accuracy_5: 0.4218 - dense_1_accuracy_6: 0.6369 - dense_1_accuracy_7: 0.8492 - dense_1_accuracy_8: 0.9637 - dense_1_accuracy_9: 0.9916 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 28/100\n",
      "358/358 [==============================] - 0s 295us/step - loss: 18.4256 - dense_1_loss: 0.0013 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7123 - dense_1_accuracy_2: 0.2039 - dense_1_accuracy_3: 0.3073 - dense_1_accuracy_4: 0.3268 - dense_1_accuracy_5: 0.4302 - dense_1_accuracy_6: 0.6480 - dense_1_accuracy_7: 0.8520 - dense_1_accuracy_8: 0.9721 - dense_1_accuracy_9: 0.9916 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 29/100\n",
      "358/358 [==============================] - 0s 299us/step - loss: 18.0490 - dense_1_loss: 0.0011 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7123 - dense_1_accuracy_2: 0.2095 - dense_1_accuracy_3: 0.3212 - dense_1_accuracy_4: 0.3380 - dense_1_accuracy_5: 0.4553 - dense_1_accuracy_6: 0.6620 - dense_1_accuracy_7: 0.8547 - dense_1_accuracy_8: 0.9749 - dense_1_accuracy_9: 0.9916 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 30/100\n",
      "358/358 [==============================] - 0s 287us/step - loss: 17.7175 - dense_1_loss: 0.0011 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7123 - dense_1_accuracy_2: 0.2151 - dense_1_accuracy_3: 0.3268 - dense_1_accuracy_4: 0.3240 - dense_1_accuracy_5: 0.4413 - dense_1_accuracy_6: 0.6620 - dense_1_accuracy_7: 0.8603 - dense_1_accuracy_8: 0.9777 - dense_1_accuracy_9: 0.9916 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 31/100\n",
      "358/358 [==============================] - 0s 292us/step - loss: 17.3973 - dense_1_loss: 0.0012 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7179 - dense_1_accuracy_2: 0.2151 - dense_1_accuracy_3: 0.3240 - dense_1_accuracy_4: 0.3743 - dense_1_accuracy_5: 0.4721 - dense_1_accuracy_6: 0.6760 - dense_1_accuracy_7: 0.8659 - dense_1_accuracy_8: 0.9749 - dense_1_accuracy_9: 0.9916 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 32/100\n",
      "358/358 [==============================] - 0s 300us/step - loss: 17.0971 - dense_1_loss: 9.2652e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7151 - dense_1_accuracy_2: 0.2179 - dense_1_accuracy_3: 0.3492 - dense_1_accuracy_4: 0.3771 - dense_1_accuracy_5: 0.4637 - dense_1_accuracy_6: 0.6760 - dense_1_accuracy_7: 0.8715 - dense_1_accuracy_8: 0.9777 - dense_1_accuracy_9: 0.9916 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 33/100\n",
      "358/358 [==============================] - 0s 299us/step - loss: 16.8054 - dense_1_loss: 9.9513e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7151 - dense_1_accuracy_2: 0.2235 - dense_1_accuracy_3: 0.3520 - dense_1_accuracy_4: 0.4022 - dense_1_accuracy_5: 0.4804 - dense_1_accuracy_6: 0.7179 - dense_1_accuracy_7: 0.8771 - dense_1_accuracy_8: 0.9777 - dense_1_accuracy_9: 0.9916 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 34/100\n",
      "358/358 [==============================] - 0s 284us/step - loss: 16.5189 - dense_1_loss: 0.0011 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7263 - dense_1_accuracy_2: 0.2235 - dense_1_accuracy_3: 0.3603 - dense_1_accuracy_4: 0.4106 - dense_1_accuracy_5: 0.5140 - dense_1_accuracy_6: 0.7095 - dense_1_accuracy_7: 0.8966 - dense_1_accuracy_8: 0.9777 - dense_1_accuracy_9: 0.9944 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 35/100\n",
      "358/358 [==============================] - 0s 282us/step - loss: 16.2461 - dense_1_loss: 9.1729e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7263 - dense_1_accuracy_2: 0.2291 - dense_1_accuracy_3: 0.3687 - dense_1_accuracy_4: 0.4302 - dense_1_accuracy_5: 0.5084 - dense_1_accuracy_6: 0.7179 - dense_1_accuracy_7: 0.8911 - dense_1_accuracy_8: 0.9777 - dense_1_accuracy_9: 0.9944 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 36/100\n",
      "358/358 [==============================] - 0s 280us/step - loss: 16.0033 - dense_1_loss: 9.1922e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7263 - dense_1_accuracy_2: 0.2291 - dense_1_accuracy_3: 0.3771 - dense_1_accuracy_4: 0.4134 - dense_1_accuracy_5: 0.5140 - dense_1_accuracy_6: 0.7263 - dense_1_accuracy_7: 0.8994 - dense_1_accuracy_8: 0.9804 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 37/100\n",
      "358/358 [==============================] - 0s 291us/step - loss: 15.7633 - dense_1_loss: 8.1421e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7318 - dense_1_accuracy_2: 0.2374 - dense_1_accuracy_3: 0.3771 - dense_1_accuracy_4: 0.4274 - dense_1_accuracy_5: 0.5140 - dense_1_accuracy_6: 0.7235 - dense_1_accuracy_7: 0.9050 - dense_1_accuracy_8: 0.9777 - dense_1_accuracy_9: 0.9944 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 38/100\n",
      "358/358 [==============================] - 0s 294us/step - loss: 15.5404 - dense_1_loss: 8.0022e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7346 - dense_1_accuracy_2: 0.2430 - dense_1_accuracy_3: 0.3966 - dense_1_accuracy_4: 0.4441 - dense_1_accuracy_5: 0.5279 - dense_1_accuracy_6: 0.7402 - dense_1_accuracy_7: 0.8939 - dense_1_accuracy_8: 0.9804 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 39/100\n",
      "358/358 [==============================] - 0s 313us/step - loss: 15.3263 - dense_1_loss: 7.6919e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7346 - dense_1_accuracy_2: 0.2291 - dense_1_accuracy_3: 0.3911 - dense_1_accuracy_4: 0.4581 - dense_1_accuracy_5: 0.5475 - dense_1_accuracy_6: 0.7458 - dense_1_accuracy_7: 0.9218 - dense_1_accuracy_8: 0.9777 - dense_1_accuracy_9: 0.9944 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 40/100\n",
      "358/358 [==============================] - 0s 291us/step - loss: 15.1289 - dense_1_loss: 8.8634e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7346 - dense_1_accuracy_2: 0.2486 - dense_1_accuracy_3: 0.3994 - dense_1_accuracy_4: 0.4609 - dense_1_accuracy_5: 0.5587 - dense_1_accuracy_6: 0.7458 - dense_1_accuracy_7: 0.9106 - dense_1_accuracy_8: 0.9804 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 41/100\n",
      "358/358 [==============================] - 0s 284us/step - loss: 14.9033 - dense_1_loss: 7.6833e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7346 - dense_1_accuracy_2: 0.2486 - dense_1_accuracy_3: 0.4022 - dense_1_accuracy_4: 0.4860 - dense_1_accuracy_5: 0.5810 - dense_1_accuracy_6: 0.7626 - dense_1_accuracy_7: 0.9162 - dense_1_accuracy_8: 0.9804 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 42/100\n",
      "358/358 [==============================] - 0s 290us/step - loss: 14.7229 - dense_1_loss: 9.6570e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7346 - dense_1_accuracy_2: 0.2514 - dense_1_accuracy_3: 0.4078 - dense_1_accuracy_4: 0.4860 - dense_1_accuracy_5: 0.6006 - dense_1_accuracy_6: 0.7570 - dense_1_accuracy_7: 0.9302 - dense_1_accuracy_8: 0.9860 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 43/100\n",
      "358/358 [==============================] - 0s 285us/step - loss: 14.5473 - dense_1_loss: 6.8687e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7346 - dense_1_accuracy_2: 0.2542 - dense_1_accuracy_3: 0.4274 - dense_1_accuracy_4: 0.4832 - dense_1_accuracy_5: 0.6257 - dense_1_accuracy_6: 0.7821 - dense_1_accuracy_7: 0.9218 - dense_1_accuracy_8: 0.9804 - dense_1_accuracy_9: 0.9944 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 44/100\n",
      "358/358 [==============================] - 0s 290us/step - loss: 14.3914 - dense_1_loss: 6.9656e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.2598 - dense_1_accuracy_3: 0.4190 - dense_1_accuracy_4: 0.4749 - dense_1_accuracy_5: 0.6034 - dense_1_accuracy_6: 0.7793 - dense_1_accuracy_7: 0.9162 - dense_1_accuracy_8: 0.9860 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 45/100\n",
      "358/358 [==============================] - 0s 289us/step - loss: 14.2335 - dense_1_loss: 6.4914e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.2570 - dense_1_accuracy_3: 0.4218 - dense_1_accuracy_4: 0.5168 - dense_1_accuracy_5: 0.6285 - dense_1_accuracy_6: 0.7737 - dense_1_accuracy_7: 0.9302 - dense_1_accuracy_8: 0.9832 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 46/100\n",
      "358/358 [==============================] - 0s 284us/step - loss: 14.0287 - dense_1_loss: 7.1169e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.2486 - dense_1_accuracy_3: 0.4385 - dense_1_accuracy_4: 0.5223 - dense_1_accuracy_5: 0.6341 - dense_1_accuracy_6: 0.7989 - dense_1_accuracy_7: 0.9469 - dense_1_accuracy_8: 0.9832 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 47/100\n",
      "358/358 [==============================] - 0s 287us/step - loss: 13.8685 - dense_1_loss: 6.6494e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.2542 - dense_1_accuracy_3: 0.4497 - dense_1_accuracy_4: 0.5279 - dense_1_accuracy_5: 0.6592 - dense_1_accuracy_6: 0.7905 - dense_1_accuracy_7: 0.9413 - dense_1_accuracy_8: 0.9860 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 48/100\n",
      "358/358 [==============================] - 0s 286us/step - loss: 13.7273 - dense_1_loss: 7.1437e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.2570 - dense_1_accuracy_3: 0.4553 - dense_1_accuracy_4: 0.5279 - dense_1_accuracy_5: 0.6788 - dense_1_accuracy_6: 0.8101 - dense_1_accuracy_7: 0.9441 - dense_1_accuracy_8: 0.9860 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 49/100\n",
      "358/358 [==============================] - 0s 286us/step - loss: 13.5967 - dense_1_loss: 5.8940e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.2626 - dense_1_accuracy_3: 0.4525 - dense_1_accuracy_4: 0.5447 - dense_1_accuracy_5: 0.6648 - dense_1_accuracy_6: 0.8156 - dense_1_accuracy_7: 0.9413 - dense_1_accuracy_8: 0.9888 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 50/100\n",
      "358/358 [==============================] - 0s 286us/step - loss: 13.4522 - dense_1_loss: 0.0011 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.2654 - dense_1_accuracy_3: 0.4581 - dense_1_accuracy_4: 0.5587 - dense_1_accuracy_5: 0.6788 - dense_1_accuracy_6: 0.8128 - dense_1_accuracy_7: 0.9497 - dense_1_accuracy_8: 0.9832 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 51/100\n",
      "358/358 [==============================] - 0s 296us/step - loss: 13.3203 - dense_1_loss: 5.9458e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.2682 - dense_1_accuracy_3: 0.4609 - dense_1_accuracy_4: 0.5670 - dense_1_accuracy_5: 0.6983 - dense_1_accuracy_6: 0.8240 - dense_1_accuracy_7: 0.9497 - dense_1_accuracy_8: 0.9832 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 52/100\n",
      "358/358 [==============================] - 0s 302us/step - loss: 13.2090 - dense_1_loss: 6.3071e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.2737 - dense_1_accuracy_3: 0.4721 - dense_1_accuracy_4: 0.5810 - dense_1_accuracy_5: 0.6983 - dense_1_accuracy_6: 0.8324 - dense_1_accuracy_7: 0.9525 - dense_1_accuracy_8: 0.9860 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 53/100\n",
      "358/358 [==============================] - 0s 289us/step - loss: 13.0970 - dense_1_loss: 5.6517e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.2682 - dense_1_accuracy_3: 0.4721 - dense_1_accuracy_4: 0.5670 - dense_1_accuracy_5: 0.7179 - dense_1_accuracy_6: 0.8464 - dense_1_accuracy_7: 0.9553 - dense_1_accuracy_8: 0.9888 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 54/100\n",
      "358/358 [==============================] - 0s 294us/step - loss: 12.9680 - dense_1_loss: 5.7530e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.2737 - dense_1_accuracy_3: 0.4777 - dense_1_accuracy_4: 0.5810 - dense_1_accuracy_5: 0.7207 - dense_1_accuracy_6: 0.8324 - dense_1_accuracy_7: 0.9581 - dense_1_accuracy_8: 0.9916 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 55/100\n",
      "358/358 [==============================] - 0s 286us/step - loss: 12.8493 - dense_1_loss: 5.3671e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.2793 - dense_1_accuracy_3: 0.4581 - dense_1_accuracy_4: 0.5866 - dense_1_accuracy_5: 0.7151 - dense_1_accuracy_6: 0.8464 - dense_1_accuracy_7: 0.9525 - dense_1_accuracy_8: 0.9944 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 56/100\n",
      "358/358 [==============================] - 0s 309us/step - loss: 12.7234 - dense_1_loss: 5.0568e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.2821 - dense_1_accuracy_3: 0.4665 - dense_1_accuracy_4: 0.5894 - dense_1_accuracy_5: 0.7430 - dense_1_accuracy_6: 0.8492 - dense_1_accuracy_7: 0.9581 - dense_1_accuracy_8: 0.9944 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 57/100\n",
      "358/358 [==============================] - 0s 306us/step - loss: 12.6125 - dense_1_loss: 5.3760e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.2849 - dense_1_accuracy_3: 0.4693 - dense_1_accuracy_4: 0.5950 - dense_1_accuracy_5: 0.7402 - dense_1_accuracy_6: 0.8436 - dense_1_accuracy_7: 0.9609 - dense_1_accuracy_8: 0.9944 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 58/100\n",
      "358/358 [==============================] - 0s 311us/step - loss: 12.5184 - dense_1_loss: 4.8317e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.2821 - dense_1_accuracy_3: 0.4916 - dense_1_accuracy_4: 0.6201 - dense_1_accuracy_5: 0.7514 - dense_1_accuracy_6: 0.8547 - dense_1_accuracy_7: 0.9637 - dense_1_accuracy_8: 0.9944 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 59/100\n",
      "358/358 [==============================] - 0s 302us/step - loss: 12.4059 - dense_1_loss: 5.0055e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.2793 - dense_1_accuracy_3: 0.4888 - dense_1_accuracy_4: 0.6089 - dense_1_accuracy_5: 0.7514 - dense_1_accuracy_6: 0.8575 - dense_1_accuracy_7: 0.9609 - dense_1_accuracy_8: 0.9944 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 60/100\n",
      "358/358 [==============================] - 0s 295us/step - loss: 12.3189 - dense_1_loss: 4.7105e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7374 - dense_1_accuracy_2: 0.2849 - dense_1_accuracy_3: 0.4916 - dense_1_accuracy_4: 0.6257 - dense_1_accuracy_5: 0.7458 - dense_1_accuracy_6: 0.8715 - dense_1_accuracy_7: 0.9665 - dense_1_accuracy_8: 0.9944 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 61/100\n",
      "358/358 [==============================] - 0s 292us/step - loss: 12.2056 - dense_1_loss: 4.6945e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.2877 - dense_1_accuracy_3: 0.4944 - dense_1_accuracy_4: 0.6201 - dense_1_accuracy_5: 0.7542 - dense_1_accuracy_6: 0.8687 - dense_1_accuracy_7: 0.9693 - dense_1_accuracy_8: 0.9972 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 62/100\n",
      "358/358 [==============================] - 0s 291us/step - loss: 12.1167 - dense_1_loss: 4.5353e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.2849 - dense_1_accuracy_3: 0.5028 - dense_1_accuracy_4: 0.6313 - dense_1_accuracy_5: 0.7626 - dense_1_accuracy_6: 0.8827 - dense_1_accuracy_7: 0.9749 - dense_1_accuracy_8: 0.9972 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 63/100\n",
      "358/358 [==============================] - 0s 296us/step - loss: 12.0440 - dense_1_loss: 4.2752e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7374 - dense_1_accuracy_2: 0.2877 - dense_1_accuracy_3: 0.4972 - dense_1_accuracy_4: 0.6257 - dense_1_accuracy_5: 0.7793 - dense_1_accuracy_6: 0.8743 - dense_1_accuracy_7: 0.9693 - dense_1_accuracy_8: 0.9944 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 64/100\n",
      "358/358 [==============================] - 0s 297us/step - loss: 11.9442 - dense_1_loss: 4.2993e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.2905 - dense_1_accuracy_3: 0.5056 - dense_1_accuracy_4: 0.6397 - dense_1_accuracy_5: 0.7793 - dense_1_accuracy_6: 0.8855 - dense_1_accuracy_7: 0.9721 - dense_1_accuracy_8: 0.9972 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 65/100\n",
      "358/358 [==============================] - 0s 290us/step - loss: 11.8836 - dense_1_loss: 4.1074e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.2905 - dense_1_accuracy_3: 0.5112 - dense_1_accuracy_4: 0.6480 - dense_1_accuracy_5: 0.7793 - dense_1_accuracy_6: 0.8771 - dense_1_accuracy_7: 0.9721 - dense_1_accuracy_8: 0.9916 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 66/100\n",
      "358/358 [==============================] - 0s 300us/step - loss: 11.7884 - dense_1_loss: 4.5653e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.2849 - dense_1_accuracy_3: 0.4972 - dense_1_accuracy_4: 0.6425 - dense_1_accuracy_5: 0.7821 - dense_1_accuracy_6: 0.8966 - dense_1_accuracy_7: 0.9721 - dense_1_accuracy_8: 0.9972 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 67/100\n",
      "358/358 [==============================] - 0s 304us/step - loss: 11.7057 - dense_1_loss: 3.9764e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.2905 - dense_1_accuracy_3: 0.5084 - dense_1_accuracy_4: 0.6369 - dense_1_accuracy_5: 0.7849 - dense_1_accuracy_6: 0.8994 - dense_1_accuracy_7: 0.9721 - dense_1_accuracy_8: 0.9944 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 68/100\n",
      "358/358 [==============================] - 0s 292us/step - loss: 11.6284 - dense_1_loss: 6.0831e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.2877 - dense_1_accuracy_3: 0.5223 - dense_1_accuracy_4: 0.6648 - dense_1_accuracy_5: 0.8045 - dense_1_accuracy_6: 0.9162 - dense_1_accuracy_7: 0.9721 - dense_1_accuracy_8: 0.9972 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 69/100\n",
      "358/358 [==============================] - 0s 285us/step - loss: 11.5516 - dense_1_loss: 3.7764e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.2933 - dense_1_accuracy_3: 0.5196 - dense_1_accuracy_4: 0.6508 - dense_1_accuracy_5: 0.7905 - dense_1_accuracy_6: 0.8966 - dense_1_accuracy_7: 0.9721 - dense_1_accuracy_8: 0.9972 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 70/100\n",
      "358/358 [==============================] - 0s 288us/step - loss: 11.4688 - dense_1_loss: 3.9573e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.2933 - dense_1_accuracy_3: 0.5279 - dense_1_accuracy_4: 0.6592 - dense_1_accuracy_5: 0.8156 - dense_1_accuracy_6: 0.9078 - dense_1_accuracy_7: 0.9721 - dense_1_accuracy_8: 0.9972 - dense_1_accuracy_9: 1.0000 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 71/100\n",
      "358/358 [==============================] - 0s 297us/step - loss: 11.3856 - dense_1_loss: 3.6547e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.2961 - dense_1_accuracy_3: 0.5223 - dense_1_accuracy_4: 0.6788 - dense_1_accuracy_5: 0.7989 - dense_1_accuracy_6: 0.9134 - dense_1_accuracy_7: 0.9693 - dense_1_accuracy_8: 0.9972 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 72/100\n",
      "358/358 [==============================] - 0s 292us/step - loss: 11.3172 - dense_1_loss: 3.5214e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.2961 - dense_1_accuracy_3: 0.5307 - dense_1_accuracy_4: 0.6704 - dense_1_accuracy_5: 0.8045 - dense_1_accuracy_6: 0.9134 - dense_1_accuracy_7: 0.9749 - dense_1_accuracy_8: 0.9972 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 73/100\n",
      "358/358 [==============================] - 0s 292us/step - loss: 11.2508 - dense_1_loss: 3.6097e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.2961 - dense_1_accuracy_3: 0.5391 - dense_1_accuracy_4: 0.6760 - dense_1_accuracy_5: 0.8101 - dense_1_accuracy_6: 0.9218 - dense_1_accuracy_7: 0.9749 - dense_1_accuracy_8: 0.9972 - dense_1_accuracy_9: 1.0000 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 74/100\n",
      "358/358 [==============================] - 0s 292us/step - loss: 11.1756 - dense_1_loss: 3.3987e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.2989 - dense_1_accuracy_3: 0.5447 - dense_1_accuracy_4: 0.6760 - dense_1_accuracy_5: 0.8268 - dense_1_accuracy_6: 0.9246 - dense_1_accuracy_7: 0.9777 - dense_1_accuracy_8: 0.9972 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 75/100\n",
      "358/358 [==============================] - 0s 283us/step - loss: 11.1240 - dense_1_loss: 3.6136e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.2989 - dense_1_accuracy_3: 0.5391 - dense_1_accuracy_4: 0.6676 - dense_1_accuracy_5: 0.8240 - dense_1_accuracy_6: 0.9190 - dense_1_accuracy_7: 0.9721 - dense_1_accuracy_8: 0.9972 - dense_1_accuracy_9: 1.0000 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 76/100\n",
      "358/358 [==============================] - 0s 284us/step - loss: 11.0618 - dense_1_loss: 3.1383e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.3045 - dense_1_accuracy_3: 0.5335 - dense_1_accuracy_4: 0.6872 - dense_1_accuracy_5: 0.8296 - dense_1_accuracy_6: 0.9302 - dense_1_accuracy_7: 0.9777 - dense_1_accuracy_8: 0.9972 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 77/100\n",
      "358/358 [==============================] - 0s 290us/step - loss: 10.9872 - dense_1_loss: 3.1285e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.3045 - dense_1_accuracy_3: 0.5419 - dense_1_accuracy_4: 0.6844 - dense_1_accuracy_5: 0.8296 - dense_1_accuracy_6: 0.9358 - dense_1_accuracy_7: 0.9749 - dense_1_accuracy_8: 0.9972 - dense_1_accuracy_9: 1.0000 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 78/100\n",
      "358/358 [==============================] - 0s 307us/step - loss: 10.9442 - dense_1_loss: 3.2068e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.3045 - dense_1_accuracy_3: 0.5363 - dense_1_accuracy_4: 0.7067 - dense_1_accuracy_5: 0.8352 - dense_1_accuracy_6: 0.9246 - dense_1_accuracy_7: 0.9777 - dense_1_accuracy_8: 0.9972 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 79/100\n",
      "358/358 [==============================] - 0s 284us/step - loss: 10.8794 - dense_1_loss: 3.2588e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.3017 - dense_1_accuracy_3: 0.5447 - dense_1_accuracy_4: 0.6927 - dense_1_accuracy_5: 0.8464 - dense_1_accuracy_6: 0.9413 - dense_1_accuracy_7: 0.9749 - dense_1_accuracy_8: 0.9972 - dense_1_accuracy_9: 1.0000 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 80/100\n",
      "358/358 [==============================] - 0s 283us/step - loss: 10.8333 - dense_1_loss: 2.8943e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.3073 - dense_1_accuracy_3: 0.5475 - dense_1_accuracy_4: 0.6983 - dense_1_accuracy_5: 0.8380 - dense_1_accuracy_6: 0.9274 - dense_1_accuracy_7: 0.9804 - dense_1_accuracy_8: 0.9972 - dense_1_accuracy_9: 1.0000 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 81/100\n",
      "358/358 [==============================] - 0s 285us/step - loss: 10.7835 - dense_1_loss: 3.1866e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.3101 - dense_1_accuracy_3: 0.5503 - dense_1_accuracy_4: 0.7039 - dense_1_accuracy_5: 0.8296 - dense_1_accuracy_6: 0.9469 - dense_1_accuracy_7: 0.9804 - dense_1_accuracy_8: 0.9972 - dense_1_accuracy_9: 1.0000 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 82/100\n",
      "358/358 [==============================] - 0s 284us/step - loss: 10.7084 - dense_1_loss: 2.8181e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.3045 - dense_1_accuracy_3: 0.5615 - dense_1_accuracy_4: 0.7095 - dense_1_accuracy_5: 0.8464 - dense_1_accuracy_6: 0.9413 - dense_1_accuracy_7: 0.9804 - dense_1_accuracy_8: 0.9972 - dense_1_accuracy_9: 1.0000 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 83/100\n",
      "358/358 [==============================] - 0s 291us/step - loss: 10.6473 - dense_1_loss: 2.9424e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.3101 - dense_1_accuracy_3: 0.5559 - dense_1_accuracy_4: 0.7235 - dense_1_accuracy_5: 0.8352 - dense_1_accuracy_6: 0.9469 - dense_1_accuracy_7: 0.9832 - dense_1_accuracy_8: 0.9972 - dense_1_accuracy_9: 0.9972 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 84/100\n",
      "358/358 [==============================] - 0s 301us/step - loss: 10.5960 - dense_1_loss: 3.1081e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7402 - dense_1_accuracy_2: 0.2989 - dense_1_accuracy_3: 0.5559 - dense_1_accuracy_4: 0.7235 - dense_1_accuracy_5: 0.8492 - dense_1_accuracy_6: 0.9497 - dense_1_accuracy_7: 0.9804 - dense_1_accuracy_8: 0.9972 - dense_1_accuracy_9: 1.0000 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 85/100\n",
      "358/358 [==============================] - 0s 300us/step - loss: 10.5457 - dense_1_loss: 2.7074e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7458 - dense_1_accuracy_2: 0.3017 - dense_1_accuracy_3: 0.5642 - dense_1_accuracy_4: 0.7207 - dense_1_accuracy_5: 0.8547 - dense_1_accuracy_6: 0.9441 - dense_1_accuracy_7: 0.9832 - dense_1_accuracy_8: 0.9972 - dense_1_accuracy_9: 1.0000 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 86/100\n",
      "358/358 [==============================] - 0s 289us/step - loss: 10.4965 - dense_1_loss: 2.9467e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7430 - dense_1_accuracy_2: 0.3101 - dense_1_accuracy_3: 0.5754 - dense_1_accuracy_4: 0.7263 - dense_1_accuracy_5: 0.8575 - dense_1_accuracy_6: 0.9497 - dense_1_accuracy_7: 0.9860 - dense_1_accuracy_8: 0.9972 - dense_1_accuracy_9: 1.0000 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 87/100\n",
      "358/358 [==============================] - 0s 286us/step - loss: 10.4534 - dense_1_loss: 2.7025e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7430 - dense_1_accuracy_2: 0.3045 - dense_1_accuracy_3: 0.5698 - dense_1_accuracy_4: 0.7318 - dense_1_accuracy_5: 0.8631 - dense_1_accuracy_6: 0.9385 - dense_1_accuracy_7: 0.9832 - dense_1_accuracy_8: 0.9972 - dense_1_accuracy_9: 1.0000 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 88/100\n",
      "358/358 [==============================] - 0s 286us/step - loss: 10.4017 - dense_1_loss: 3.0360e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7430 - dense_1_accuracy_2: 0.3101 - dense_1_accuracy_3: 0.5810 - dense_1_accuracy_4: 0.7291 - dense_1_accuracy_5: 0.8631 - dense_1_accuracy_6: 0.9553 - dense_1_accuracy_7: 0.9888 - dense_1_accuracy_8: 0.9972 - dense_1_accuracy_9: 1.0000 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 89/100\n",
      "358/358 [==============================] - 0s 279us/step - loss: 10.3578 - dense_1_loss: 2.6649e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7430 - dense_1_accuracy_2: 0.3128 - dense_1_accuracy_3: 0.5810 - dense_1_accuracy_4: 0.7430 - dense_1_accuracy_5: 0.8659 - dense_1_accuracy_6: 0.9469 - dense_1_accuracy_7: 0.9832 - dense_1_accuracy_8: 1.0000 - dense_1_accuracy_9: 1.0000 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 90/100\n",
      "358/358 [==============================] - 0s 286us/step - loss: 10.3060 - dense_1_loss: 2.6875e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7430 - dense_1_accuracy_2: 0.3101 - dense_1_accuracy_3: 0.5866 - dense_1_accuracy_4: 0.7318 - dense_1_accuracy_5: 0.8743 - dense_1_accuracy_6: 0.9525 - dense_1_accuracy_7: 0.9832 - dense_1_accuracy_8: 1.0000 - dense_1_accuracy_9: 1.0000 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 91/100\n",
      "358/358 [==============================] - 0s 291us/step - loss: 10.2595 - dense_1_loss: 2.5962e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7430 - dense_1_accuracy_2: 0.3128 - dense_1_accuracy_3: 0.5922 - dense_1_accuracy_4: 0.7402 - dense_1_accuracy_5: 0.8659 - dense_1_accuracy_6: 0.9553 - dense_1_accuracy_7: 0.9860 - dense_1_accuracy_8: 0.9972 - dense_1_accuracy_9: 1.0000 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 92/100\n",
      "358/358 [==============================] - 0s 298us/step - loss: 10.2202 - dense_1_loss: 2.5913e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7458 - dense_1_accuracy_2: 0.3128 - dense_1_accuracy_3: 0.5866 - dense_1_accuracy_4: 0.7402 - dense_1_accuracy_5: 0.8799 - dense_1_accuracy_6: 0.9637 - dense_1_accuracy_7: 0.9860 - dense_1_accuracy_8: 0.9972 - dense_1_accuracy_9: 1.0000 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 93/100\n",
      "358/358 [==============================] - 0s 289us/step - loss: 10.1758 - dense_1_loss: 2.5391e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7458 - dense_1_accuracy_2: 0.3101 - dense_1_accuracy_3: 0.5978 - dense_1_accuracy_4: 0.7458 - dense_1_accuracy_5: 0.8855 - dense_1_accuracy_6: 0.9609 - dense_1_accuracy_7: 0.9888 - dense_1_accuracy_8: 0.9972 - dense_1_accuracy_9: 1.0000 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 94/100\n",
      "358/358 [==============================] - 0s 293us/step - loss: 10.1293 - dense_1_loss: 2.4402e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7486 - dense_1_accuracy_2: 0.3073 - dense_1_accuracy_3: 0.5866 - dense_1_accuracy_4: 0.7374 - dense_1_accuracy_5: 0.8855 - dense_1_accuracy_6: 0.9609 - dense_1_accuracy_7: 0.9860 - dense_1_accuracy_8: 1.0000 - dense_1_accuracy_9: 1.0000 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 95/100\n",
      "358/358 [==============================] - 0s 290us/step - loss: 10.0878 - dense_1_loss: 2.3386e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7486 - dense_1_accuracy_2: 0.3128 - dense_1_accuracy_3: 0.5922 - dense_1_accuracy_4: 0.7514 - dense_1_accuracy_5: 0.8883 - dense_1_accuracy_6: 0.9609 - dense_1_accuracy_7: 0.9916 - dense_1_accuracy_8: 0.9972 - dense_1_accuracy_9: 1.0000 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 96/100\n",
      "358/358 [==============================] - 0s 301us/step - loss: 10.0434 - dense_1_loss: 2.3873e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7486 - dense_1_accuracy_2: 0.3156 - dense_1_accuracy_3: 0.6089 - dense_1_accuracy_4: 0.7486 - dense_1_accuracy_5: 0.8966 - dense_1_accuracy_6: 0.9609 - dense_1_accuracy_7: 0.9888 - dense_1_accuracy_8: 0.9972 - dense_1_accuracy_9: 1.0000 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 97/100\n",
      "358/358 [==============================] - 0s 318us/step - loss: 10.0045 - dense_1_loss: 2.2722e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7486 - dense_1_accuracy_2: 0.3101 - dense_1_accuracy_3: 0.6117 - dense_1_accuracy_4: 0.7542 - dense_1_accuracy_5: 0.8939 - dense_1_accuracy_6: 0.9637 - dense_1_accuracy_7: 0.9888 - dense_1_accuracy_8: 0.9972 - dense_1_accuracy_9: 1.0000 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 98/100\n",
      "358/358 [==============================] - 0s 290us/step - loss: 9.9641 - dense_1_loss: 2.1715e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7486 - dense_1_accuracy_2: 0.3073 - dense_1_accuracy_3: 0.6089 - dense_1_accuracy_4: 0.7570 - dense_1_accuracy_5: 0.8966 - dense_1_accuracy_6: 0.9609 - dense_1_accuracy_7: 0.9888 - dense_1_accuracy_8: 1.0000 - dense_1_accuracy_9: 1.0000 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 99/100\n",
      "358/358 [==============================] - 0s 296us/step - loss: 9.9336 - dense_1_loss: 2.4065e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7486 - dense_1_accuracy_2: 0.3156 - dense_1_accuracy_3: 0.6089 - dense_1_accuracy_4: 0.7626 - dense_1_accuracy_5: 0.8911 - dense_1_accuracy_6: 0.9665 - dense_1_accuracy_7: 0.9888 - dense_1_accuracy_8: 0.9972 - dense_1_accuracy_9: 1.0000 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n",
      "Epoch 100/100\n",
      "358/358 [==============================] - 0s 299us/step - loss: 9.8953 - dense_1_loss: 2.1737e-04 - dense_1_accuracy: 0.1844 - dense_1_accuracy_1: 0.7486 - dense_1_accuracy_2: 0.3128 - dense_1_accuracy_3: 0.6089 - dense_1_accuracy_4: 0.7598 - dense_1_accuracy_5: 0.9022 - dense_1_accuracy_6: 0.9665 - dense_1_accuracy_7: 0.9916 - dense_1_accuracy_8: 0.9972 - dense_1_accuracy_9: 1.0000 - dense_1_accuracy_10: 1.0000 - dense_1_accuracy_11: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x14bb5c520>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X, a0, c0], list(Y), epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Generating moonboard problem\n",
    "\n",
    "You now have a trained model which has learned the patterns of the jazz soloist. Lets now use this model to synthesize new music. \n",
    "\n",
    "#### 3.1 - Predicting & Sampling\n",
    "\n",
    "<img src=\"images/music_gen.png\" style=\"width:600;height:400px;\">\n",
    "\n",
    "At each step of sampling, you will:\n",
    "* Take as input the activation '`a`' and cell state '`c`' from the previous state of the LSTM.\n",
    "* Forward propagate by one step.\n",
    "* Get a new output activation as well as cell state. \n",
    "* The new activation '`a`' can then be used to generate the output using the fully connected layer, `densor`. \n",
    "\n",
    "##### Initialization\n",
    "* We will initialize the following to be zeros:\n",
    "    * `x0` \n",
    "    * hidden state `a0` \n",
    "    * cell state `c0` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** \n",
    "* Implement the function below to sample a sequence of musical values. \n",
    "* Here are some of the key steps you'll need to implement inside the for-loop that generates the $T_y$ output characters: \n",
    "\n",
    "* Step 2.A: Use `LSTM_Cell`, which takes in the input layer, as well as the previous step's '`c`' and '`a`' to generate the current step's '`c`' and '`a`'. \n",
    "```Python\n",
    "next_hidden_state, _, next_cell_state = LSTM_cell(input_x, initial_state=[previous_hidden_state, previous_cell_state])\n",
    "```\n",
    "    * Choose the appropriate variables for the input_x, hidden_state, and cell_state\n",
    "\n",
    "* Step 2.B: Compute the output by applying `densor` to compute a softmax on '`a`' to get the output for the current step. \n",
    "\n",
    "* Step 2.C: Append the output to the list `outputs`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 2.D: Sample x to be the one-hot version of '`out`'. \n",
    "* This allows you to pass it to the next LSTM's step.  \n",
    "* We have provided the definition of `one_hot(x)` in the 'music_utils.py' file and imported it.\n",
    "Here is the definition of `one_hot`\n",
    "```Python\n",
    "def one_hot(x):\n",
    "    x = K.argmax(x)\n",
    "    x = tf.one_hot(indices=x, depth=237) \n",
    "    x = RepeatVector(1)(x)\n",
    "    return x\n",
    "```\n",
    "Here is what the `one_hot` function is doing:\n",
    "* argmax: within the vector `x`, find the position with the maximum value and return the index of that position.  \n",
    "    * For example: argmax of [-1,0,1] finds that 1 is the maximum value, and returns the index position, which is 2.  Read the documentation for [keras.argmax](https://www.tensorflow.org/api_docs/python/tf/keras/backend/argmax).\n",
    "* one_hot: takes a list of indices and the depth of the one-hot vector (number of categories, which is 237 in this assignment).  It converts each index into the one-hot vector representation.  For instance, if the indices is [2], and the depth is 5, then the one-hot vector returned is [0,0,1,0,0].  Check out the documentation for [tf.one_hot](https://www.tensorflow.org/api_docs/python/tf/one_hot) for more examples and explanations.\n",
    "* RepeatVector(n): This takes a vector and duplicates it `n` times.  Notice that we had it repeat 1 time.  This may seem like it's not doing anything.  If you look at the documentation for [RepeatVector](https://keras.io/layers/core/#repeatvector), you'll notice that if x is a vector with dimension (m,5) and it gets passed into `RepeatVector(1)`, then the output is (m,1,5).  In other words, it adds an additional dimension (of length 1) to the resulting vector.\n",
    "* Apply the custom one_hot encoding using the [Lambda](https://keras.io/layers/core/#lambda) layer.  You saw earlier that the Lambda layer can be used like this:\n",
    "```Python\n",
    "result = Lambda(lambda x: x + 1)(input_var)\n",
    "```\n",
    "\n",
    "If you pre-define a function, you can do the same thing:\n",
    "```Python\n",
    "def add_one(x)\n",
    "    return x + 1\n",
    "\n",
    "# use the add_one function inside of the Lambda function\n",
    "result = Lambda(add_one)(input_var)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Inference Model:  \n",
    "This is how to use the Keras `Model`.\n",
    "```Python\n",
    "model = Model(inputs=[input_x, initial_hidden_state, initial_cell_state], outputs=the_outputs)\n",
    "```\n",
    "\n",
    "\n",
    "* Choose the appropriate variables for the input tensor, hidden state, cell state, and output.\n",
    "* **Hint**: the inputs to the model are the **initial** inputs and states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x):\n",
    "    x = K.argmax(x)\n",
    "    x = tf.one_hot(x, 237) \n",
    "    x = RepeatVector(1)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: music_inference_model\n",
    "\n",
    "def music_inference_model(LSTM_cell, densor, n_values = 237, n_a = 64, Ty = 12):\n",
    "    \"\"\"\n",
    "    Uses the trained \"LSTM_cell\" and \"densor\" from model() to generate a sequence of values.\n",
    "    \n",
    "    Arguments:\n",
    "    LSTM_cell -- the trained \"LSTM_cell\" from model(), Keras layer object\n",
    "    densor -- the trained \"densor\" from model(), Keras layer object\n",
    "    n_values -- integer, number of unique values\n",
    "    n_a -- number of units in the LSTM_cell\n",
    "    Ty -- integer, number of time steps to generate\n",
    "    \n",
    "    Returns:\n",
    "    inference_model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input of your model with a shape \n",
    "    x0 = Input(shape=(1, n_values))\n",
    "    \n",
    "    # Define s0, initial hidden state for the decoder LSTM\n",
    "    a0 = Input(shape=(n_a,), name='a0')\n",
    "    c0 = Input(shape=(n_a,), name='c0')\n",
    "    a = a0\n",
    "    c = c0\n",
    "    x = x0\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Create an empty list of \"outputs\" to later store your predicted values (≈1 line)\n",
    "    outputs = []\n",
    "    \n",
    "    # Step 2: Loop over Ty and generate a value at every time step\n",
    "    for t in range(Ty):\n",
    "        \n",
    "        # Step 2.A: Perform one step of LSTM_cell (≈1 line)\n",
    "        a, _, c = LSTM_cell(x, initial_state=[a, c])\n",
    "        \n",
    "        # Step 2.B: Apply Dense layer to the hidden state output of the LSTM_cell (≈1 line)\n",
    "        out = densor(a)\n",
    "\n",
    "        # Step 2.C: Append the prediction \"out\" to \"outputs\". out.shape = (None, 237) (≈1 line)\n",
    "        outputs.append(out)\n",
    "        \n",
    "        # Step 2.D: \n",
    "        # Select the next value according to \"out\",\n",
    "        # Set \"x\" to be the one-hot representation of the selected value\n",
    "        # See instructions above.\n",
    "        x = Lambda(one_hot)(out)\n",
    "        \n",
    "        \n",
    "    # Step 3: Create model instance with the correct \"inputs\" and \"outputs\" (≈1 line)\n",
    "    inference_model = Model(inputs = [x0, a0, c0], outputs = outputs)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return inference_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to define your inference model. This model is hard coded to generate 50 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model = music_inference_model(LSTM_cell, densor, n_values = 237, n_a = 64, Ty = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 1, 237)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "a0 (InputLayer)                 (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 64), (None,  77312       input_3[0][0]                    \n",
      "                                                                 a0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 lambda_13[0][0]                  \n",
      "                                                                 lstm_1[13][0]                    \n",
      "                                                                 lstm_1[13][2]                    \n",
      "                                                                 lambda_14[0][0]                  \n",
      "                                                                 lstm_1[14][0]                    \n",
      "                                                                 lstm_1[14][2]                    \n",
      "                                                                 lambda_15[0][0]                  \n",
      "                                                                 lstm_1[15][0]                    \n",
      "                                                                 lstm_1[15][2]                    \n",
      "                                                                 lambda_16[0][0]                  \n",
      "                                                                 lstm_1[16][0]                    \n",
      "                                                                 lstm_1[16][2]                    \n",
      "                                                                 lambda_17[0][0]                  \n",
      "                                                                 lstm_1[17][0]                    \n",
      "                                                                 lstm_1[17][2]                    \n",
      "                                                                 lambda_18[0][0]                  \n",
      "                                                                 lstm_1[18][0]                    \n",
      "                                                                 lstm_1[18][2]                    \n",
      "                                                                 lambda_19[0][0]                  \n",
      "                                                                 lstm_1[19][0]                    \n",
      "                                                                 lstm_1[19][2]                    \n",
      "                                                                 lambda_20[0][0]                  \n",
      "                                                                 lstm_1[20][0]                    \n",
      "                                                                 lstm_1[20][2]                    \n",
      "                                                                 lambda_21[0][0]                  \n",
      "                                                                 lstm_1[21][0]                    \n",
      "                                                                 lstm_1[21][2]                    \n",
      "                                                                 lambda_22[0][0]                  \n",
      "                                                                 lstm_1[22][0]                    \n",
      "                                                                 lstm_1[22][2]                    \n",
      "                                                                 lambda_23[0][0]                  \n",
      "                                                                 lstm_1[23][0]                    \n",
      "                                                                 lstm_1[23][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 237)          15405       lstm_1[13][0]                    \n",
      "                                                                 lstm_1[14][0]                    \n",
      "                                                                 lstm_1[15][0]                    \n",
      "                                                                 lstm_1[16][0]                    \n",
      "                                                                 lstm_1[17][0]                    \n",
      "                                                                 lstm_1[18][0]                    \n",
      "                                                                 lstm_1[19][0]                    \n",
      "                                                                 lstm_1[20][0]                    \n",
      "                                                                 lstm_1[21][0]                    \n",
      "                                                                 lstm_1[22][0]                    \n",
      "                                                                 lstm_1[23][0]                    \n",
      "                                                                 lstm_1[24][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 1, 237)       0           dense_1[13][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 1, 237)       0           dense_1[14][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 1, 237)       0           dense_1[15][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 1, 237)       0           dense_1[16][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, 1, 237)       0           dense_1[17][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_18 (Lambda)              (None, 1, 237)       0           dense_1[18][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_19 (Lambda)              (None, 1, 237)       0           dense_1[19][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_20 (Lambda)              (None, 1, 237)       0           dense_1[20][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_21 (Lambda)              (None, 1, 237)       0           dense_1[21][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 1, 237)       0           dense_1[22][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_23 (Lambda)              (None, 1, 237)       0           dense_1[23][0]                   \n",
      "==================================================================================================\n",
      "Total params: 92,717\n",
      "Trainable params: 92,717\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Check the inference model\n",
    "inference_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected Output**\n",
    "If you scroll to the bottom of the output, you'll see:\n",
    "```\n",
    "Total params: 41,678\n",
    "Trainable params: 41,678\n",
    "Non-trainable params: 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize inference model\n",
    "The following code creates the zero-valued vectors you will use to initialize `x` and the LSTM state variables `a` and `c`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_initializer = np.zeros((1, 1, 237))\n",
    "#a_initializer = np.zeros((1, n_a))\n",
    "c_initializer = np.zeros((1, n_a))\n",
    "#x_initializer = np.random.rand(1, 1, 237)\n",
    "a_initializer = np.random.rand(1, n_a)\n",
    "c_initializer = np.random.rand(1, n_a) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Implement `predict_and_sample()`. \n",
    "\n",
    "* This function takes many arguments including the inputs [x_initializer, a_initializer, c_initializer]. \n",
    "* In order to predict the output corresponding to this input, you will need to carry-out 3 steps:\n",
    "\n",
    "\n",
    "#### Step 1\n",
    "* Use your inference model to predict an output given your set of inputs. The output `pred` should be a list of length $T_y$ where each element is a numpy-array of shape (1, n_values).\n",
    "```Python\n",
    "inference_model.predict([input_x_init, hidden_state_init, cell_state_init])\n",
    "```\n",
    "    * Choose the appropriate input arguments to `predict` from the input arguments of this `predict_and_sample` function.\n",
    " \n",
    "#### Step 2\n",
    "* Convert `pred` into a numpy array of $T_y$ indices. \n",
    "    * Each index is computed by taking the `argmax` of an element of the `pred` list. \n",
    "    * Use [numpy.argmax](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html).\n",
    "    * Set the `axis` parameter.\n",
    "        * Remember that the shape of the prediction is $(m, T_{y}, n_{values})$\n",
    "\n",
    "#### Step 3  \n",
    "* Convert the indices into their one-hot vector representations. \n",
    "    * Use [to_categorical](https://keras.io/utils/#to_categorical).\n",
    "    * Set the `num_classes` parameter. Note that for grading purposes: you'll need to either:\n",
    "        * Use a dimension from the given parameters of `predict_and_sample()` (for example, one of the dimensions of x_initializer has the value for the number of distinct classes).\n",
    "        * Or just hard code the number of distinct classes (will pass the grader as well).\n",
    "        * Note that using a global variable such as n_values will not work for grading purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict_and_sample\n",
    "\n",
    "def predict_and_sample(inference_model, x_initializer = x_initializer, a_initializer = a_initializer, \n",
    "                       c_initializer = c_initializer):\n",
    "    \"\"\"\n",
    "    Predicts the next value of values using the inference model.\n",
    "    \n",
    "    Arguments:\n",
    "    inference_model -- Keras model instance for inference time\n",
    "    x_initializer -- numpy array of shape (1, 1, 237), one-hot vector initializing the values generation\n",
    "    a_initializer -- numpy array of shape (1, n_a), initializing the hidden state of the LSTM_cell\n",
    "    c_initializer -- numpy array of shape (1, n_a), initializing the cell state of the LSTM_cel\n",
    "    \n",
    "    Returns:\n",
    "    results -- numpy-array of shape (Ty, 237), matrix of one-hot vectors representing the values generated\n",
    "    indices -- numpy-array of shape (Ty, 1), matrix of indices representing the values generated\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Use your inference model to predict an output sequence given x_initializer, a_initializer and c_initializer.\n",
    "    pred = inference_model.predict([x_initializer, a_initializer, c_initializer])\n",
    "    # Step 2: Convert \"pred\" into an np.array() of indices with the maximum probabilities\n",
    "    indices =  np.argmax(pred, axis = 2)\n",
    "    # Step 3: Convert indices to one-hot vectors, the shape of the results should be (Ty, n_values)\n",
    "    results =  to_categorical(indices, num_classes = np.shape(x_initializer)[2])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return results, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x7f539b681d30>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.argmax(results[2]) = 142\n",
      "np.argmax(results[7]) = 0\n",
      "list(indices[2:8]) = [array([142]), array([204]), array([88]), array([114]), array([1]), array([0])]\n"
     ]
    }
   ],
   "source": [
    "results, indices = predict_and_sample(inference_model, x_initializer, a_initializer, c_initializer)\n",
    "print(\"np.argmax(results[2]) =\", np.argmax(results[2]))\n",
    "print(\"np.argmax(results[7]) =\", np.argmax(results[7]))\n",
    "print(\"list(indices[2:8]) =\", list(indices[2:8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepRouteSet generated problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By runing following two cells, you can now insert those into moonboard app and create new problems to see the layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.argmax(results[2]) = 191\n",
      "np.argmax(results[7]) = 0\n",
      "list(indices[2:8]) = [array([191]), array([195]), array([89]), array([114]), array([1]), array([0])]\n"
     ]
    }
   ],
   "source": [
    "x_initializer = np.zeros((1, 1, 237))\n",
    "a_initializer = np.random.rand(1, n_a)\n",
    "c_initializer = np.random.rand(1, n_a) / 2\n",
    "\n",
    "results, indices = predict_and_sample(inference_model, x_initializer, a_initializer, c_initializer)\n",
    "print(\"np.argmax(results[2]) =\", np.argmax(results[2]))\n",
    "print(\"np.argmax(results[7]) =\", np.argmax(results[7]))\n",
    "print(\"list(indices[2:8]) =\", list(indices[2:8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepRouteSet's generated problem:\n",
      "A5-LH\n",
      "A5-RH\n",
      "A5-RH\n",
      "B8-LH\n",
      "F10-RH\n",
      "E16-LH\n",
      "I18-RH\n",
      "End\n",
      "End\n",
      "End\n",
      "End\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "print(\"DeepRouteSet's generated problem:\")\n",
    "for i in range(12):\n",
    "    print(holdIx_to_holdStr[int(indices[i])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Space to improve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    *Unlike music generation, don't allow the same hold, same hand happen consequtively.\n",
    "    *Initial conditions like x,a,c can be changed\n",
    "    *Should compare the similarity with the training set. \n",
    "    *Enlarge the training set. Now it is only ~300 benchmark problems\n",
    "    *Put a final filter to filter out problem with no ?18-LH / ?18-RH \n",
    "    *Package the DeepRouteSet with gradeNet to predict the grade\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Space to improve**: \n",
    "\n",
    "* Unlike music generation, don't allow the same hold, same hand happen consequtively.\n",
    "* Initial conditions like x,a,c can be changed\n",
    "* Should compare the similarity with the training set. \n",
    "* Enlarge the training set. Now it is only ~300 benchmark problems\n",
    "* Put a final filter to filter out problem with no ?18-LH / ?18-RH \n",
    "* Package the DeepRouteSet with gradeNet to predict the grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected (Approximate) Output**: \n",
    "\n",
    "* Your results **may likely differ** because Keras' results are not completely predictable. \n",
    "* However, if you have trained your LSTM_cell with model.fit() for exactly 100 epochs as described above: \n",
    "    * You should very likely observe a sequence of indices that are not all identical. \n",
    "    * Moreover, you should observe that: \n",
    "        * np.argmax(results[12]) is the first element of list(indices[12:18]) \n",
    "        * and np.argmax(results[17]) is the last element of list(indices[12:18]). \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **np.argmax(results[12])** =\n",
    "        </td>\n",
    "        <td>\n",
    "        1\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **np.argmax(results[17])** =\n",
    "        </td>\n",
    "        <td>\n",
    "        42\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **list(indices[12:18])** =\n",
    "        </td>\n",
    "        <td>\n",
    "            [array([1]), array([42]), array([54]), array([17]), array([1]), array([42])]\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 - Generate moonboard problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations!\n",
    "\n",
    "You have come to the end of the notebook. \n",
    "\n",
    "\n",
    "## What you should remember\n",
    "- A sequence model can be used to generate musical values, which are then post-processed into midi music. \n",
    "- Fairly similar models can be used to generate dinosaur names or to generate music, with the major difference being the input fed to the model.  \n",
    "- In Keras, sequence generation involves defining layers with shared weights, which are then repeated for the different time steps $1, \\ldots, T_x$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "The ideas presented in this notebook came primarily from three computational music papers cited below. The implementation here also took significant inspiration and used many components from Ji-Sung Kim's GitHub repository.\n",
    "\n",
    "- Ji-Sung Kim, 2016, [deepjazz](https://github.com/jisungk/deepjazz)\n",
    "- Jon Gillick, Kevin Tang and Robert Keller, 2009. [Learning Jazz Grammars](http://ai.stanford.edu/~kdtang/papers/smc09-jazzgrammar.pdf)\n",
    "- Robert Keller and David Morrison, 2007, [A Grammatical Approach to Automatic Improvisation](http://smc07.uoa.gr/SMC07%20Proceedings/SMC07%20Paper%2055.pdf)\n",
    "- François Pachet, 1999, [Surprising Harmonies](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.5.7473&rep=rep1&type=pdf)\n",
    "\n",
    "We're also grateful to François Germain for valuable feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "EG0F7",
   "launcher_item_id": "cxJXc"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
